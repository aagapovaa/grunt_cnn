{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d2a36-c6f0-4844-a6ab-664323a9bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Код нейронной сети, модификация для данных RGB. \n",
    "\n",
    "#Установка нужных библиотек. Использовалась версия PyTorch 2.3.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 \n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.nn import Linear, Conv2d, ReLU, Sigmoid, MaxPool2d, BatchNorm2d, Dropout, Flatten, Sequential\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e0e4f-d1fc-4e10-8918-e6f6af286537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Аугментация данных\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=(40, 40), antialias=True, scale=(0.7, 1.0)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=(-180, 180)), \n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=(-15, 15)),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    v2.RandomAutocontrast(p=0.5),\n",
    "    v2.RandomEqualize(p=0.1),\n",
    "    v2.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "val_test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(size=(40, 40), antialias=True),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca7c96-a219-494d-8bc4-c11ea7e50f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(cnn_model, self).__init__()\n",
    "\n",
    "\n",
    "        self.batch_size = shape[0]\n",
    "        self.channels = shape[1]\n",
    "        self.height = shape[2]\n",
    "        self.width = shape[3]\n",
    "        \n",
    "        conv_layers = [Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            padding_mode='reflect',\n",
    "            ),\n",
    "        BatchNorm2d(32),\n",
    "        ReLU (inplace = True),\n",
    "        Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=32, \n",
    "            kernel_size=3, \n",
    "            padding='same', \n",
    "            padding_mode='reflect',\n",
    "            ),\n",
    "        BatchNorm2d(32),\n",
    "        ReLU(inplace=True),               \n",
    "        MaxPool2d(kernel_size=2),\n",
    "        Dropout(0.2),\n",
    "               \n",
    "        Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            padding_mode='reflect',\n",
    "        ),\n",
    "        BatchNorm2d(64),\n",
    "        ReLU(inplace = True),\n",
    "        Conv2d(\n",
    "            in_channels=64, \n",
    "            out_channels=64, \n",
    "            kernel_size=3, \n",
    "            padding='same', \n",
    "            padding_mode='reflect'),\n",
    "        BatchNorm2d(64),\n",
    "        ReLU(inplace=True),\n",
    "        MaxPool2d(kernel_size=2),\n",
    "        Dropout(0.3),\n",
    "               \n",
    "        Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            padding_mode='reflect',\n",
    "            ),\n",
    "        BatchNorm2d(128),\n",
    "        ReLU(inplace = True),\n",
    "        Conv2d(\n",
    "            in_channels=128, \n",
    "            out_channels=128, \n",
    "            kernel_size=3, \n",
    "            padding='same', \n",
    "            padding_mode='reflect'),\n",
    "        BatchNorm2d(128),\n",
    "        ReLU(inplace=True),\n",
    "        MaxPool2d(kernel_size=2),\n",
    "        Dropout(0.4),\n",
    "        ]\n",
    "\n",
    "        linear_layers = [Linear(3200 , 256), ReLU(inplace=True), Dropout(0.5), Linear(256, 64), ReLU(inplace=True), Linear(64, 1), Sigmoid()]\n",
    "\n",
    "        layers = conv_layers + [Flatten()] + linear_layers\n",
    "        self.model = Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def train_model(self, train_loader, val_loader=None, epochs=100, patience=10):\n",
    "        metrics = {\n",
    "                'train': {'loss': [], 'accuracy': []},\n",
    "                'val': {'loss': [], 'accuracy': []}\n",
    "        }\n",
    "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.to(device)\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        no_improvement = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            ep_metrics = {\n",
    "                'train': {'loss': 0, 'accuracy': 0, 'count': 0},\n",
    "                'val': {'loss': 0, 'accuracy': 0, 'count': 0},\n",
    "            }\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            self.model.train()\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                        \n",
    "                output = self(images).view(-1)\n",
    "                loss = criterion(output, labels)\n",
    "                        \n",
    "                correct_preds = (output > 0.65).float() == labels\n",
    "                accuracy = correct_preds.sum()/len(labels)\n",
    "                        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                ep_metrics['train']['loss'] += loss.item()\n",
    "                ep_metrics['train']['accuracy'] += accuracy.item()\n",
    "                ep_metrics['train']['count'] += 1\n",
    "                \n",
    "            if val_loader:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in val_loader:\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.float().to(device)\n",
    "                        \n",
    "                        output = self(images).view(-1)\n",
    "                        loss = criterion(output, labels)\n",
    "                        \n",
    "                        correct_preds = (output > 0.65).float() == labels\n",
    "                        accuracy = correct_preds.sum() / len(labels)\n",
    "                        \n",
    "                        ep_metrics['val']['loss'] += loss.item()\n",
    "                        ep_metrics['val']['accuracy'] += accuracy.item()\n",
    "                        ep_metrics['val']['count'] += 1\n",
    "                \n",
    "                train_loss = ep_metrics['train']['loss']/ep_metrics['train']['count']\n",
    "                train_acc = ep_metrics['train']['accuracy']/ep_metrics['train']['count']\n",
    "\n",
    "                print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "            \n",
    "                metrics['train']['loss'].append(train_loss)\n",
    "                metrics['train']['accuracy'].append(train_acc)\n",
    "            \n",
    "                if val_loader:\n",
    "                    val_loss = ep_metrics['val']['loss'] / ep_metrics['val']['count']\n",
    "                    val_acc = ep_metrics['val']['accuracy'] / ep_metrics['val']['count']\n",
    "                \n",
    "                    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "                \n",
    "                    metrics['val']['loss'].append(val_loss)\n",
    "                    metrics['val']['accuracy'].append(val_acc)\n",
    "            \n",
    "                    scheduler.step(val_loss)\n",
    "                \n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        no_improvement = 0\n",
    "                        torch.save(self.state_dict(), 'best_road_model.pth')\n",
    "                    else:\n",
    "                        no_improvement += 1\n",
    "                \n",
    "                    if no_improvement >= patience:\n",
    "                        print(f'Early stopping at epoch {epoch+1}')\n",
    "                        break\n",
    "                print('End')\n",
    "                \n",
    "            if val_loader:\n",
    "                self.load_state_dict(torch.load('best_road_model.pth'))\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self = self.to(device=device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        tot_loss = 0\n",
    "        tot_acc = 0\n",
    "        count = 0\n",
    "        \n",
    "        preds = []\n",
    "        actual = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                output = self(images).view(-1)\n",
    "                loss = criterion(output, labels.float())\n",
    "                \n",
    "                pred_labels = (output > 0.5).float()\n",
    "                accuracy = (labels.float() == pred_labels).sum() / len(labels)\n",
    "                \n",
    "                tot_loss += loss.item()\n",
    "                tot_acc += accuracy.item()\n",
    "                count += 1\n",
    "                \n",
    "                preds.extend(pred_labels.cpu().numpy())\n",
    "                actual.extend(labels.cpu().numpy())\n",
    "        \n",
    "        print(f\"Test Loss: {tot_loss / count:.4f}, Test Accuracy: {tot_acc / count:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'loss': tot_loss / count,\n",
    "            'accuracy': tot_acc / count,\n",
    "            'predictions': preds,\n",
    "            'actual': actual}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32878a-00bd-42ac-b598-039e8ad6c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция загрузки обучающей выборки\n",
    "set_path = 'set_path' #Заменить на путь к набору данных\n",
    "def load_and_prepare_data(data_dir, batch_size=32):\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=set_path, transform=train_transforms)\n",
    "    \n",
    "    train_size = int(0.7 * len(train_dataset))\n",
    "    val_size = int(0.15 * len(train_dataset))\n",
    "    test_size = len(train_dataset) - train_size - val_size\n",
    "    \n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(train_dataset, [train_size, val_size, test_size] )\n",
    "    \n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=set_path, transform=val_test_transforms)\n",
    "    \n",
    "    val_indices = val_set.indices\n",
    "    val_set = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "    \n",
    "    test_dataset = torchvision.datasets.ImageFolder(root=set_path, transform=val_test_transforms)\n",
    "    \n",
    "    test_indices = test_set.indices\n",
    "    test_set = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        \"test\": test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983b91e-995e-4735-bded-83a153b029c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создание графиков для обучения\n",
    "def plot_training_history(metrics, save_path):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(metrics['train']['loss'], label='Обучающая выборка', color = 'purple')\n",
    "    ax1.plot(metrics['val']['loss'], label='Валидационная выборка', color = 'pink')\n",
    "    ax1.set_title('Значения функции потерь в зависимости от эпохи обучения')\n",
    "    ax1.set_xlabel('Эпохи')\n",
    "    ax1.set_ylabel('Функция потерь')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(metrics['train']['accuracy'], label='Обучающая выборка', color = 'purple')\n",
    "    ax2.plot(metrics['val']['accuracy'], label='Валидационная выборка', color = 'brown')\n",
    "    ax2.set_title('Значения точности (accuracy) в зависимости от эпохи обучения')\n",
    "    ax2.set_xlabel('Эпохи')\n",
    "    ax2.set_ylabel('Точность')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, save_path):\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='RdPu')\n",
    "    plt.title('Матрица ошибок')\n",
    "    plt.ylabel('Фактический класс')\n",
    "    plt.xlabel('Предсказанный класс')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae4ed3-cb8a-47c7-979c-4983dca486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "\n",
    "results_dir = 'results_path' #Заменить на директорию для результатов обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320e9b7-38c8-46eb-9728-48930e6732c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучение модели\n",
    "def main():\n",
    "    data_loaders = load_and_prepare_data(set_path, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    shape = (BATCH_SIZE, 3, 40, 40)\n",
    "    model = cnn_model(shape)\n",
    "    \n",
    "    \n",
    "    metrics = model.train_model(\n",
    "        train_loader=data_loaders[\"train\"],\n",
    "        val_loader=data_loaders[\"val\"],\n",
    "        epochs=EPOCHS,\n",
    "        patience=PATIENCE\n",
    "    )\n",
    "    \n",
    "    plot_training_history(metrics, os.path.join(results_dir, \"training_history.png\"))\n",
    "    \n",
    "    test_results = model.evaluate(data_loaders[\"test\"])\n",
    "    \n",
    "    y_true = test_results['actual']\n",
    "    y_pred = test_results['predictions']\n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    with open(os.path.join(results_dir, \"classification_report.txt\"), 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results['loss']:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {test_results['accuracy']:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(class_report)\n",
    "    \n",
    "    plot_confusion_matrix(y_true, y_pred, os.path.join(results_dir, \"confusion_matrix.png\"))\n",
    "    \n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loaders[\"test\"]:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(images).view(-1)\n",
    "            \n",
    "            all_probs.extend(output.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(results_dir, \"final_model.pth\"))\n",
    "    \n",
    "    print(f\"\\nОбучение завершено. Результаты сохранены в директории: {results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ebe27-b2f5-4fa3-83a4-3cbdd98a8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Код, применяющий обученную модель, расположенную по пути final_model, к изображению\n",
    "\n",
    "class PatchBasedInference:\n",
    "    def __init__(self, cnn_model, patch_size=40, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "        self.model = cnn_model\n",
    "        self.patch_size = patch_size\n",
    "        self.device = device\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.model.eval()\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize(size=(patch_size, patch_size), antialias=True),\n",
    "            v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "        \n",
    "    def load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB') \n",
    "        image_np = np.array(image)\n",
    "        return image, image_np\n",
    "    \n",
    "    def extract_patches(self, image_np):\n",
    "        height, width, channels = image_np.shape\n",
    "        stride = self.patch_size - 20 #Параметр перекрытия патчей (20 = 50%). Настраивается в зависимости от результатов (но может привести к большим времязатратам\n",
    "        \n",
    "        patches = []\n",
    "        positions = []\n",
    "        \n",
    "        for y in range(0, height, stride):\n",
    "            for x in range(0, width, stride):\n",
    "                y1 = y\n",
    "                x1 = x\n",
    "                y2 = min(y1 + self.patch_size, height)\n",
    "                x2 = min(x1 + self.patch_size, width)\n",
    "\n",
    "                \n",
    "                patch = patch = image_np[y1:y2, x1:x2]\n",
    "\n",
    "                if patch.shape[0] != self.patch_size or patch.shape[1] != self.patch_size:\n",
    "                    temp_patch = np.zeros((self.patch_size, self.patch_size, 3), dtype=np.uint8)\n",
    "                    temp_patch[:patch.shape[0], :patch.shape[1], :] = patch\n",
    "                    patch = temp_patch\n",
    "                \n",
    "                patches.append(patch)\n",
    "                positions.append((y1, x1, y2, x2))\n",
    "        \n",
    "        return patches, positions\n",
    "    \n",
    "    def process_patches(self, patches):\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for patch in patches:\n",
    "                patch_pil = Image.fromarray(patch)\n",
    "                patch_tensor = self.transform(patch_pil).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                output = self.model.model(patch_tensor).view(-1)\n",
    "                confidence = output.item()\n",
    "                pred = (confidence > 0.5) #Значение 0.5 может меняться в большую сторону, если результат зашумлен\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                confidences.append(confidence)\n",
    "        \n",
    "        return predictions, confidences\n",
    "\n",
    "    \n",
    "    def create_visualization(self, predictions, confidences, positions, original_image):\n",
    "        visualization = np.zeros(original_image.shape, dtype=np.uint8)\n",
    "\n",
    "        for pred, conf, (y1, x1, y2, x2) in zip(predictions, confidences, positions):\n",
    "            if pred:\n",
    "                color = [255, 255, 255]\n",
    "            else:\n",
    "                color = [0, 0, 0]\n",
    "\n",
    "            for c in range(3):\n",
    "                visualization[y1:y2, x1:x2, c] = color[c]\n",
    "    \n",
    "        return np.clip(visualization, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def process_image(self, image_path, output_path=True, show_result=True):\n",
    "        image, image_np = self.load_image(image_path)\n",
    "        \n",
    "        patches, positions = self.extract_patches(image_np)\n",
    "        \n",
    "        predictions, confidences = self.process_patches(patches)\n",
    "\n",
    "        result_image = None\n",
    "        \n",
    "        result_image = self.create_visualization(predictions, confidences, positions, image_np)\n",
    "        \n",
    "        if result_image.ndim == 4:\n",
    "            result_image = np.squeeze(result_image)\n",
    "        result_image = result_image.astype(np.uint8)\n",
    "\n",
    "        if output_path:\n",
    "            Image.fromarray(result_image).save(output_path)\n",
    "\n",
    "        if show_result:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(result_image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        return result_image\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 40, 40)\n",
    "model = cnn_model(input_shape)\n",
    "model.load_state_dict(torch.load(final_model))\n",
    "model.model.eval()\n",
    "\n",
    "processor = PatchBasedInference(model, patch_size=40)\n",
    "\n",
    "\n",
    "image_path = 'image_path' #П  \n",
    "output_path = r'C:\\kursach\\applied\\other_plots\\results\\rlipetsk_2.jpg'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
